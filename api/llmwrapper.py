import os
from cerebras.cloud.sdk import Cerebras


class LLMWrapper:
    """
    A wrapper for interacting with Cerebras's LLM API for generating summaries or other language model tasks.
    Now supports a system prompt and the ability to maintain message history.
    """

    def __init__(self, model="llama3.1-8b", api_key="csk-dkcdcf8tk988r82vjdke62fc99dm4mw38cx6c99nrdrypjj8", system_prompt=None):
        """
        Initializes the LLM wrapper.
        :param model: The LLM model to use (default: llama3.1-8b).
        :param api_key: Cerebras API key for authentication (optional, will retrieve from environment if not provided).
        :param system_prompt: Optional system prompt to guide the model's behavior.
        """
        self.client = Cerebras(api_key=api_key or os.environ.get("CEREBRAS_API_KEY"))
        self.model = model
        self.system_prompt = system_prompt
        self.history = []  # Stores chat history for ongoing interactions

    def summarize(self, prompt):
        """
        Summarizes the provided content using the LLM with a specific prompt.
        :param prompt: The text content to summarize.
        :return: The summary generated by the LLM.
        """
        messages = [{"role": "user", "content": prompt}]
        if self.system_prompt:
            messages.insert(0, {"role": "system", "content": self.system_prompt})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        return response.choices[0].message.content

    def chat_with_history(self, user_input):
        """
        Chat with the model while maintaining conversation history.
        :param user_input: The user input to be added to the ongoing conversation.
        :return: The LLM's response while taking the message history into account.
        """
        if not self.history and self.system_prompt:
            self.history.append({"role": "system", "content": self.system_prompt})

        self.history.append({"role": "user", "content": user_input})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.history
        )

        model_response = response.choices[0].message.content
        self.history.append({"role": "assistant", "content": model_response})

        return model_response


if __name__ == "__main__":
    # Example of usage
    system_prompt = "You are a helpful assistant. Summarize the following content concisely."
    llm = LLMWrapper(system_prompt=system_prompt)

    # Example input to summarize
    prompt = "Here is a list of changes: created backend routing with Django, worked on React frontend."
    summary = llm.summarize(prompt)
    print("Summary:", summary)

    # Start a conversation with history
    response_1 = llm.chat_with_history("What are the advantages of fast inference in AI?")
    print(f"Assistant: {response_1}")

    response_2 = llm.chat_with_history("Can you explain more about how inference works?")
    print(f"Assistant: {response_2}")
